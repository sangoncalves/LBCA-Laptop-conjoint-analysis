laptops$RAM         <- factor(laptops$RAM, levels=c( "4GB", "8GB", "16GB", "32GB" ))
laptops$Memory      <- factor(laptops$Memory, levels=c( "126GB", "256GB", "512GB", "1T" ))
laptops$Processor   <- factor(laptops$Processor, levels=c( "i3", "i5", "i7", "i9" ))
laptops$Weight      <- factor(laptops$Weight, levels=c( "0.8kg", "1kg", "1.2kg", "1.5kg" ))
laptops$ScreenSize  <- as.factor(laptops$ScreenSize)
laptops$alt         <- factor(laptops$alt, levels=c("1", "2", "3", "4")) # Not sure if we need to convert alt to qualitative
#Fitting model
laptops.mlogit <- dfidx(laptops, idx = list(c("ques", "resp.id"), "alt"), drop.index=F,levels=c(1,2,3,4))
#Creating alternatives for the models to be chosen (lm = laptop_model)
lm1 <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize, data = laptops.mlogit)
summary(lm1)
lm2 <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit)
summary(lm2)
lrtest(lm2, lm1) #compare the larger model with intercepts and the nested smaller model without them in order to identify if we can use the model without the intercepts to gain more precision. Slide 20. In our case p-value is 0.5133, if we use level of significance 0.05, then we do not have enough evidence to reject the null hypothesis, therefore the model can be seen as identical.
#Theory about the model
{
#The Estimate column provides the estimated average part worth for each level; they have to be interpreted with respect to the reference level of each attribute
#The order of magnitude of the estimates provides how strong the preferences are. MNL model coefficients are on the logit scale, they tend to range mainly between −2 and 2
#The Std. Error column gives the level of precision of the estimates and is followed by the z-test statistics and associated p-value indicating the sample evidence against the null hypothesis that the coefficient is equal to zero.
#The estimated intercepts, that represent the so-called alternative specific   constants (asc), provide the preferences for the positions of the alternatives in each question
# estimated alternative specific constants are not significantly different from zero-> consumers not choosing based on the location of the option
#If we find one of the intercepts to be significant, that may imply that some respondents chose the mid or the right option while disregarding the question.
}
#Analyze model with price as qualitative vs quantitative
lm3 <- mlogit(choice ~  as.numeric(as.character(Price)) + RAM + Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit)
summary(lm3)
lrtest(lm3, lm2)
#since our p-value is smaller than our significance level(0.05), we cannot use price as quantitative variable. This means that we cannot analyze the willingness-to-pay for each level’s attribute.
##Can we analyze the willingness-to-pay on each price????
###########We need to have attention regarding to the preference share! There are some restrictions in interpreting the results, need to check the related class video!
#it is important to not treat the obtained preference share predictions as actual market share forecasts. Indeed, while these predictions represent well the respondents behaviour in a survey context, they not necessarily translate to actual sales in the real marketplace.
#Considering the set of relevant designs vs all possible sets
attributes <- list(Price=names(table(laptops.mlogit$Price)),
RAM=names(table(laptops.mlogit$RAM)),
Memory=names(table(laptops.mlogit$Memory)),
Processor=names(table(laptops.mlogit$Processor)),
Weight=names(table(laptops.mlogit$Weight)),
ScreenSize=names(table(laptops.mlogit$ScreenSize)))
allDesign <- expand.grid(attributes)
allDesign #all possible design
# In order to choose the designs, it is a good approach to remove the ones that are not feasible. High end features with lowest price for example.
# we need to choose some designs to analyze, the possible combinations are too great, we need to reduce them in a meaningful way, like some for entry, mid and high end market.
ProductSelection <- function(Price,RAM,Memory,Processor,Weight,ScreenSize){
ram <-  paste(as.character(RAM), "GB", sep = "")
if(Memory==1) memory <- paste(as.character(Memory), "T", sep = "") else memory <-  paste(as.character(Memory), "GB", sep = "")
processor <-  paste('i',as.character(Processor), sep = "")
weight <-  paste(as.character(Weight), "kg", sep = "")
return(filter(allDesign, Price == {{Price}}, RAM == {{ram}}, Memory == {{ memory }}, Processor == {{ processor }}, Weight == {{ weight }}, ScreenSize == {{ ScreenSize }}))
}
#Entry market
entry1 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 126,Processor = 3, Weight = 0.8,ScreenSize = 12)
entry2 <- ProductSelection(Price = 1, RAM = 4,Memory = 126,Processor = 3, Weight = 1.2,ScreenSize = 13)
#Mid market
mid1 <- ProductSelection(Price = 1, RAM = 8,Memory = 256,Processor = 5, Weight = 1, ScreenSize = 13)
mid2 <- ProductSelection(Price = 1.5, RAM = 16,Memory = 512,Processor = 7, Weight = 1.5,ScreenSize = 16)
#High end market
high1 <- ProductSelection(Price = 2, RAM = 16,Memory = 512, Processor = 7, Weight = 1.2,ScreenSize = 16)
high2 <- ProductSelection(Price = 2, RAM = 32,Memory = 1, Processor = 9, Weight = 1.5,ScreenSize = 14)
profiles <- rbind(entry1, entry2, mid1, mid2, high1, high2)
predict.mnl <- function(model, data) {
# Function for predicting preference shares from a MNL model
# model: mlogit object returned by mlogit()
# data: a data frame containing the set of designs for which you want to
#       predict shares.  Same format at the data used to estimate model.
data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]
logitUtility <- data.model%*%model$coef
share <- exp(logitUtility)/sum(exp(logitUtility))
cbind(share, data)
}
predict.mnl(lm2, profiles) # using m2 specification
getmode <- function(v) {
uniqv <- unique(v) %>% select(colnames(laptops)-c("resp.id","qes", "alt"))
uniqv[which.max(tabulate(match(v, uniqv)))]
}
x <- getmode(laptops)
sensitivity.mnl <- function(model, attrib, base.data, competitor.data) {
# Function for creating data for a preference share-sensitivity chart
# model: mlogit object returned by mlogit() function
# attrib: list of vectors with attribute levels to be used in sensitivity
# base.data: data frame containing baseline design of target product
# competitor.data: data frame contining design of competitive set
data <- rbind(base.data, competitor.data)
base.share <- predict.mnl(model, data)[1,1]
share <- NULL
for (a in seq_along(attrib)) {
for (i in attrib[[a]]) {
data[1,] <- base.data
data[1,a] <- i
share <- c(share, predict.mnl(model, data)[1,1])
}
}
data.frame(level=unlist(attrib), share=share, increase=share-base.share)
}
base.data <- profiles[5,]
competitor.data <- profiles[-5,]
tradeoff <- sensitivity.mnl(lm2, attributes, base.data, competitor.data)
barplot(tradeoff$increase, horiz=FALSE, names.arg=tradeoff$level,
ylab="Change in Share for the Planned Product Design",
ylim=c(-0.1,0.4))
grid(nx=NA, ny=NULL)
#heterogeneity
lm2.rpar <- rep("n", length=length(lm2$coef))
names(lm2.rpar) <- names(lm2$coef)
lm2.rpar
lm2.mixed <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize  | -1,
data = laptops.mlogit,
panel=TRUE, rpar = lm2.rpar, correlation = FALSE)
summary(lm2.mixed)
# We can get a visual summary of the distribution of random effects and hence of the level of heterogeneity
layout(matrix(c(3,3,2,3), 2, 2, byrow = TRUE))
#par(mfrow=c(2,2))
plot(lm2.mixed)
#comparing the sign of the quantiles we can identify that Processori5 and Weight1.5kg have different signs, which could imply into heterogeneity
par(mfrow=c(2,2))
processor5.distr <- rpar(lm2.mixed, "Processori5")
summary(processor5.distr)
mean(processor5.distr)
med(processor5.distr)
plot(processor5.distr)
Weight1.5kg.distr <- rpar(lm2.mixed, "Weight1.5kg")
summary(Weight1.5kg.distr)
mean(Weight1.5kg.distr)
med(Weight1.5kg.distr)
plot(Weight1.5kg.distr)
#High significance means 3*
lm2.mixed2 <- update(lm2.mixed, correlation = TRUE)
summary(lm2.mixed2)
cov2cor(cov.mlogit(lm2.mixed2))
summary(vcov(lm2.mixed2, what = "rpar", type = "cor"))
lm2.mixed3 <- update(lm2.mixed2, correlation = c("Price1.5", "RAM8GB","RAM16GB", "RAM32GB", "Memory256GB","Memory512GB", "Memory1T", "Processori5", "Processori7","Processori9", "Weight1kg", "Weight1.2kg", "Weight1.5kg", "ScreenSize13", "ScreenSize14", "ScreenSize16"))
# The significant presence of random coefficients and their correlation
# can be further investigated using the ML tests, such as the ML ratio test
lrtest(lm2, lm2.mixed) #Fixed effects vs. uncorrelated random effects
lrtest(lm2.mixed, lm2.mixed2) #Uncorrelated random effects vs. all correlated random effects
lrtest(lm2.mixed3, lm2.mixed2) #partially correlated random effects vs. all correlated random effects
# Simulating shares
library(MASS)
predict.mixed.mnl <- function(model, data, nresp=1000) {
# Function for predicting shares from a mixed MNL model
# model: mlogit object returned by mlogit()
# data: a data frame containing the set of designs for which you want to
#       predict shares. Same format at the data used to estimate model.
# Note that this code assumes all model parameters are random
data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]
coef.Sigma <- cov.mlogit(model)
coef.mu <- model$coef[1:dim(coef.Sigma)[1]]
draws <- mvrnorm(n=nresp, coef.mu, coef.Sigma)
shares <- matrix(NA, nrow=nresp, ncol=nrow(data))
for (i in 1:nresp) {
utility <- data.model%*%draws[i,]
share = exp(utility)/sum(exp(utility))
shares[i,] <- share
}
cbind(colMeans(shares), data)
}
set.seed(1111)
predict.mixed.mnl(lm2.mixed2, data=profiles)
library(DescTools)
# Indexing
laptops.chosen <- filter(laptops,laptops$choice == "1")
laptops.indexed <- laptops.chosen
laptops.indexed$id <- paste(as.character(laptops.indexed$Price),
as.character(laptops.indexed$RAM),
as.character(laptops.indexed$Memory),
as.character(laptops.indexed$Processor),
as.character(laptops.indexed$Weight),
as.character(laptops.indexed$ScreenSize), sep = "")
# Profiles more "popular" (top chosen)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>%  arrange(desc(Freq)) %>% top_n(15, Freq)
head(df)
df <- df %>% as.data.frame() %>%  arrange(desc(Freq)) %>% top_n(15, Freq)
# Profiles more "popular" (top chosen)
library(dplyr)
df <- df %>% as.data.frame() %>%  arrange(desc(Freq)) %>% top_n(15, Freq)
df
## Plot
library(ggplot2)
library(forcats)
theme_set(theme_classic())
g <- ggplot(top15_profiles, aes(Var1, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
g <- ggplot(df, aes(Var1, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
g <- ggplot(df, aes(Var1, asc(Freq)))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df %>% arrange(desc(Freq)) )  %>% ggplot(df, aes(Var1, Freq)) + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df %>% arrange(desc(Freq))  %>% ggplot(df, aes(Var1, Freq)) + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df %>% mutate(Var1 = fct_reorder(Var1, Freq)) %>% ggplot(df, aes(Var1, Freq)) + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq)  %>% arrange(desc(Freq))
head(df)
theme_set(theme_classic())
g <- ggplot(df, aes(Var1, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df %>% arrange(val) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(name=factor(name, levels=name)) %>%
ggplot(aes(Var1, Freq))+ geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df %>% arrange(val) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(name=factor(name, levels=name)) %>%
ggplot(aes(Var1, Freq))
df %>% arrange(Var1) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(name=factor(name, levels=name)) %>%
ggplot(aes(Var1, Freq))
df %>% arrange(Freq) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(Var1=factor(Var1, levels=Var1)) %>%
ggplot(aes(Var1, Freq))
df %>% arrange(Freq) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(Var1 = fct_reorder(Var1, Freq)) %>%
ggplot(aes(Var1, Freq))
df %>% arrange(Freq) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(Var1 = fct_reorder(Var1, Freq)) %>%
ggplot(aes(Var1, Freq))
df %>% arrange(Var1) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(Freq = fct_reorder(Freq, Var1)) %>%
ggplot(aes(Var1, Freq))
df %>% arrange(Freq) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
mutate(Var1 = fct_reorder(Var1, Freq)) %>%
ggplot(aes(Freq, Var1))
df
df <- df %>% as.data.frame() %>% top_n(15, Freq)  %>% arrange(desc(Freq))
df
df2 <- df %>% as.data.frame() %>% top_n(15, Freq)  %>% arrange(desc(Freq))
df2
df <- df[1:16]
df <- df[1:16,]
df
df <- df %>% as.data.frame() %>% top_n(15, Freq)  %>% arrange(desc(Freq))
df <- df[1:15,]
theme_set(theme_classic())
g <- ggplot(df, aes(Var1, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df2 <- transform(df, Var1=reorder(Var1, -Freq) )
df2
g <- ggplot(df2, aes(Var1, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
# Indexing
laptops.chosen <- filter(laptops,laptops$choice == "1")
laptops.indexed <- laptops.chosen
laptops.indexed$id <- paste(as.character(laptops.indexed$Price),".",
as.character(laptops.indexed$RAM),".",
as.character(laptops.indexed$Memory),".",
as.character(laptops.indexed$Processor),".",
as.character(laptops.indexed$Weight),".",
as.character(laptops.indexed$ScreenSize), sep = "")
# Profiles more "popular" (top chosen)
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq)
df <- df[1:15,]
df <- transform(df, Var1=reorder(Var1, -Freq))
## Plot
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(df2, aes(Var1, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)
df
# Profiles more "popular" (top chosen)
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)
df <- df[1:15,]
df <- transform(df, Profiles=reorder(Profiles, -Freq))
## Plot
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(df2, aes(Profiles, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
# Profiles more "popular" (top chosen)
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)
df <- df[1:15,]
df <- transform(df, Profiles=reorder(Profiles, -Freq))
## Plot
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(df, aes(Profiles, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
laptops.chosen <- filter(laptops,laptops$choice == "1")
laptops.indexed <- laptops.chosen
laptops.indexed$id <- paste(as.character(laptops.indexed$Price),"|",
as.character(laptops.indexed$RAM),"|",
as.character(laptops.indexed$Memory),"|",
as.character(laptops.indexed$Processor),"|",
as.character(laptops.indexed$Weight),"|",
as.character(laptops.indexed$ScreenSize), sep = "")
# Profiles more "popular" (top chosen)
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)
df <- df[1:15,]
df <- transform(df, Profiles=reorder(Profiles, -Freq))
## Plot
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(df, aes(Profiles, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
laptops.chosen <- filter(laptops,laptops$choice == "1")
laptops.indexed <- laptops.chosen
laptops.indexed$id <- paste(as.character(laptops.indexed$Price),":",
as.character(laptops.indexed$RAM),":",
as.character(laptops.indexed$Memory),":",
as.character(laptops.indexed$Processor),":",
as.character(laptops.indexed$Weight),":",
as.character(laptops.indexed$ScreenSize), sep = "")
# Profiles more "popular" (top chosen)
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)
df <- df[1:15,]
df <- transform(df, Profiles=reorder(Profiles, -Freq))
## Plot
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(df, aes(Profiles, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
laptops.indexed$id <- paste(as.character(laptops.indexed$Price),"-",
as.character(laptops.indexed$RAM),"-",
as.character(laptops.indexed$Memory),"-",
as.character(laptops.indexed$Processor),"-",
as.character(laptops.indexed$Weight),"-",
as.character(laptops.indexed$ScreenSize), sep = "")
# Profiles more "popular" (top chosen)
library(dplyr)
freqtable <- table(laptops.indexed$id)
df <- as.data.frame.table(freqtable)
df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)
df <- df[1:15,]
df <- transform(df, Profiles=reorder(Profiles, -Freq))
## Plot
library(ggplot2)
theme_set(theme_classic())
g <- ggplot(df, aes(Profiles, Freq))
g + geom_bar(stat="identity", width = 0.5, fill="tomato2") +
labs(title="Profiles counting",
caption="Frequency of profiles") +
theme(axis.text.x = element_text(angle=65, vjust=0.6))
remotes::install_github('yihui/xaringan')
#Analysing if the data is balanced
summary(laptops)
xtabs(choice ~ Price, data=laptops)
xtabs(choice ~ RAM, data=laptops)
xtabs(choice ~ Memory, data=laptops)
xtabs(choice ~ Processor, data=laptops)
xtabs(choice ~ Weight, data=laptops)
xtabs(choice ~ ScreenSize, data=laptops)
install.packages("xaringanthemer")
install.packages("pagedown")
################################################################
### Analysis of Choice Based Conjoint survey data            ###
### The Multinomial Logit and Mixed Multinomial Logit models ###
################################################################
# set the directory where the data are located
setwd("C:\\Users\\sande\\OneDrive\\Masters Data Science\\3? Semester\\Laboratory of Consumer and Business Analytics\\Project\\Choice-based Conjoint Analysis the use of the MNL model\\Practice2_Choice_Based_Conjoint_MNL_model")
# load library for fitting multinomial logit models
library(mlogit)
# import the data about the Minivans Survey for conjoint analysis
minivan <- read.csv("Minivan_Choice.csv", sep=";")
head(minivan)
# see some descriptive statistics
summary(minivan)
xtabs(choice ~ price, data=minivan)
xtabs(choice ~ cargo, data=minivan)
xtabs(choice ~ seat, data=minivan)
xtabs(choice ~ engine, data=minivan)
# recode some variables
minivan$seat <- as.factor(minivan$seat) # convert the variable as qualitative
minivan$price <- as.factor(minivan$price) # convert the variable as qualitative
minivan$engine <- factor(minivan$engine, levels=c("gas","hyb","elec")) # change order of categories
# Fitting a choice model with "mlogit" function
# mlogit requires the choice data to be in a special data format created using the
# dfidx() function. You pass your choice data to dfidx, along
# with a few parameters telling it how the data is organized.
# dfidx() accepts data in either a ?long? or a ?wide? format and you tell it
# which you have using the shape parameter.
minivan.mlogit <- dfidx(minivan, idx = list(c("ques", "resp.id"), "alt"), drop.index=F,
levels=c("left", "center", "right"))
# The resulting minivan.mlogit is an mlogit.data object that can be used to estimate
# a model with mlogit(). The syntax for mlogit uses formula notation
# similarly to other functions for regression models in R.
# However, it requires the use of symbol "|" to distinguish between alternative-specific
# and non-alternative specific variables, see ?Formula
m1 <- mlogit(choice ~ seat + cargo + engine + price, data = minivan.mlogit)
summary(m1)
# Fit the model without intercept parameters
m2 <- mlogit(choice ~ seat + cargo + engine + price | -1, data = minivan.mlogit)
summary(m2)
# Test the restriction on the intercepts by comparing the two models
# through a likelihood ratio test
lrtest(m2, m1)
# Fit the model without intercept parameters and with price as a quantitative variable
m3 <- mlogit(choice ~ seat + cargo + engine
+ as.numeric(as.character(price)) | -1, data = minivan.mlogit)
summary(m3)
lrtest(m3, m2)
# Compute the willingness to pay
coef(m3)["engineelec"]/(coef(m3)["as.numeric(as.character(price))"]/1000)
# Simulate preference shares using the "predict.mnl" function
# Define the function
predict.mnl <- function(model, data) {
# Function for predicting preference shares from a MNL model
# model: mlogit object returned by mlogit()
# data: a data frame containing the set of designs for which you want to
#       predict shares.  Same format at the data used to estimate model.
data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]
logitUtility <- data.model%*%model$coef
share <- exp(logitUtility)/sum(exp(logitUtility))
cbind(share, data)
}
# In order to use "predict.mnl", you need to define a data frame containing the set of designs
# for which you want to predict the preference shares.
# One way to do this is to create the full set of possible designs
# using expand.grid() and select the designs we want by row number
attributes <- list(seat=names(table(minivan.mlogit$seat)),
cargo=names(table(minivan.mlogit$cargo)),
engine=names(table(minivan.mlogit$engine)),
price=names(table(minivan.mlogit$price)))
allDesign <- expand.grid(attributes)
allDesign #all possible design
# we choose a reasonable and realistic subset (where the first row indicates our design), such as
new.data <- allDesign[c(8, 1, 3, 41, 49, 26), ]
new.data
# We then pass these designs to predict.mnl() to determine what customers
# would choose if they had to pick among these six minivan alternatives:
predict.mnl(m3, new.data) # using m3 specification
predict.mnl(m2, new.data) # using m2 specification
# Compute and plot preference share sensitivity
# Producing a sensitivity chart using R is relatively simple: we just need to loop through all
# the attribute levels, compute a preference share prediction, and save the predicted preference share for
# the target design. The "sensitivity.mnl" function does that.
sensitivity.mnl <- function(model, attrib, base.data, competitor.data) {
# Function for creating data for a preference share-sensitivity chart
# model: mlogit object returned by mlogit() function
# attrib: list of vectors with attribute levels to be used in sensitivity
# base.data: data frame containing baseline design of target product
# competitor.data: data frame contining design of competitive set
data <- rbind(base.data, competitor.data)
base.share <- predict.mnl(model, data)[1,1]
share <- NULL
for (a in seq_along(attrib)) {
for (i in attrib[[a]]) {
data[1,] <- base.data
data[1,a] <- i
share <- c(share, predict.mnl(model, data)[1,1])
}
}
data.frame(level=unlist(attrib), share=share, increase=share-base.share)
}
base.data <- new.data[1,]
competitor.data <- new.data[-1,]
(tradeoff <- sensitivity.mnl(m2, attributes, base.data, competitor.data))
barplot(tradeoff$increase, horiz=FALSE, names.arg=tradeoff$level,
ylab="Change in Share for the Planned Product Design",
ylim=c(-0.1,0.11))
grid(nx=NA, ny=NULL)
# set the directory where the data are located
setwd("C:\\Users\\sande\\OneDrive\\Masters Data Science\\3? Semester\\Laboratory of Consumer and Business Analytics\\Project\\Choice-based Conjoint Analysis the use of the MNL model\\Practice2_Choice_Based_Conjoint_MNL_model")
