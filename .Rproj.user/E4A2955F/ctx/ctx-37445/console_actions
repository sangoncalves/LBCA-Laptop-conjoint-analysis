{
    "type": [
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        0,
        1,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        2,
        2,
        2,
        2,
        0,
        1,
        2,
        3,
        3,
        0,
        1,
        2,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        1,
        2
    ],
    "data": [
        "> ",
        "knitr::opts_chunk$set(echo = TRUE)",
        "> ",
        "library(mlogit)",
        "> ",
        "library(dplyr)",
        "> ",
        "library(ggplot2)",
        "> ",
        "library(MASS)",
        "> ",
        "library(lattice)",
        "> ",
        "library(parallel)",
        "> ",
        "laptops = read.csv(\"Dataset/laptops.csv\", sep=\";\")",
        "> ",
        "",
        "> ",
        "laptops$Price       <- as.factor(laptops$Price)",
        "> ",
        "laptops$RAM         <- factor(laptops$RAM, levels=c( \"4GB\", \"8GB\", \"16GB\", \"32GB\" ))",
        "> ",
        "laptops$Memory      <- factor(laptops$Memory, levels=c( \"126GB\", \"256GB\", \"512GB\", \"1T\" ))",
        "> ",
        "laptops$Processor   <- factor(laptops$Processor, levels=c( \"i3\", \"i5\", \"i7\", \"i9\" ))",
        "> ",
        "laptops$Weight      <- factor(laptops$Weight, levels=c( \"0.8kg\", \"1kg\", \"1.2kg\", \"1.5kg\" ))",
        "> ",
        "laptops$ScreenSize  <- as.factor(laptops$ScreenSize) ",
        "> ",
        "laptops$alt         <- factor(laptops$alt, levels=c(\"1\", \"2\", \"3\", \"4\"))",
        "> ",
        "summary(laptops)",
        "    resp.id            ques      alt      Price        RAM         Memory     Processor   Weight     ScreenSize\n Min.   :  1.00   Min.   :   1   1:7500   0.7:7516   4GB :7582   126GB:7432   i3:7491   0.8kg:7464   12:7584   \n 1st Qu.: 75.75   1st Qu.:1876   2:7500   1  :7411   8GB :7319   256GB:7596   i5:7580   1kg  :7372   13:7487   \n Median :150.50   Median :3750   3:7500   1.5:7545   16GB:7609   512GB:7524   i7:7352   1.2kg:7584   14:7509   \n Mean   :150.50   Mean   :3750   4:7500   2  :7528   32GB:7490  ",
        " 1T   :7448   i9:7577   1.5kg:7580   16:7420   \n 3rd Qu.:225.25   3rd Qu.:5625                                                                                 \n Max.   :300.00   Max.   :7500                                                                                 \n     choice    \n Min.   :0.00  \n 1st Qu.:0.00  \n Median :0.00  \n Mean   :0.25  \n 3rd Qu.:0.25  \n Max.   :1.00  \n",
        "> ",
        "# To check balance, he used this function",
        "> ",
        "sapply(laptops, table)",
        "$resp.id\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n 59  60  61  62  63  64  65  66  67 ",
        " 68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n 88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ",
        "137 138 139 140 141 142 143 144 145 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 \n100 100 ",
        "100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 \n100 100 100 100 100 100 100 100 100 100 100 100 100 ",
        "100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n291 292 293 294 295 296 297 298 299 300 \n100 100 100 100 100 100 100 100 100 100 \n\n$ques\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23 \n   4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n  24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n  47   48   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64   65   66   67   68   69 \n   4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4 \n  70   71   72   73   74   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   90   91   92 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n  93   94   95   96   97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4 \n 116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 162  163  164  165  166  167  168  169  170 ",
        " 171  172  173  174  175  176  177  178  179  180  181  182  183  184 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226 ",
        " 227  228  229  230 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274  275  276 \n   4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4 \n 323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 369  370  371 ",
        " 372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 415  416  417  418  419  420  421  422  423  424  425  426  427 ",
        " 428  429  430  431  432  433  434  435  436  437 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 438  439  440  441  442  443  444  445  446  447  448  449  450  451  452  453  454  455  456  457  458  459  460 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483 ",
        "\n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 484  485  486  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 507  508  509  510  511  512  513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528  529 \n   4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4 \n 530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547  548  549  550  551  552 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 553  554  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4 \n 576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592  593  594  595  596  597  598 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616  617  618  619  620  621 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 622  623  624  625  626  627  628 ",
        " 629  630  631  632  633  634  635  636  637  638  639  640  641  642  643  644 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 645  646  647  648  649  650  651  652  653  654  655  656  657  658  659  660  661  662  663  664  665  666  667 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 668  669  670  671  672  673  674  675  676  677  678  679  680  681  682  683  684 ",
        " 685  686  687  688  689  690 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 691  692  693  694  695  696  697  698  699  700  701  702  703  704  705  706  707  708  709  710  711  712  713 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 714  715  716  717  718  719  720  721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752  753  754  755  756  757  758  759 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 760  761  762  763  764  765  766  767  768  769  770  771  772  773  774  775  776  777  778  779  780  781  782 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4 \n 783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 806  807  808  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824  825  826  827  828 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 829 ",
        " 830  831  832  833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  871  872  873  874 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 875  876  877  878  879  880  881  882  883  884  885 ",
        " 886  887  888  889  890  891  892  893  894  895  896  897 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 898  899  900  901  902  903  904  905  906  907  908  909  910  911  912  913  914  915  916  917  918  919  920 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941 ",
        " 942  943 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 967  968  969  970  971  972  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989 \n   4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 990  991  992  993  994  995  996  997  998  999 1000 \n   4    4    4    4    4    4    4    4    4    4    4 \n [ reached getOption(\"max.print\") -- omitted 6500 entries ]\n\n$alt\n\n   1    2    3    4 \n7500 7500 7500 7500 \n\n$Price\n\n 0.7    1  1.5    2 \n7516 7411 7545 7528 \n\n$RAM\n\n 4GB  8GB 16GB 32GB \n7582 7319 7609 7490 \n\n$Memory\n\n126GB 256GB 512GB    1T \n 7432  7596  7524  7448 \n\n$Processor\n\n  i3   i5   i7   i9 \n7491 7580 7352 7577 \n",
        "\n$Weight\n\n0.8kg   1kg 1.2kg 1.5kg \n 7464  7372  7584  7580 \n\n$ScreenSize\n\n  12   13   14   16 \n7584 7487 7509 7420 \n\n$choice\n\n    0     1 \n22500  7500 \n\n",
        "> ",
        "xtabs(choice ~ Price, data=laptops)",
        "Price\n 0.7    1  1.5    2 \n2338 1895 1773 1494 \n",
        "> ",
        "xtabs(choice ~ RAM, data=laptops)",
        "RAM\n 4GB  8GB 16GB 32GB \n1988 2865 1347 1300 \n",
        "> ",
        "xtabs(choice ~ Memory, data=laptops)",
        "Memory\n126GB 256GB 512GB    1T \n 2043  1127  1153  3177 \n",
        "> ",
        "xtabs(choice ~ Processor, data=laptops)",
        "Processor\n  i3   i5   i7   i9 \n2121 2329 1861 1189 \n",
        "> ",
        "xtabs(choice ~ Weight, data=laptops)",
        "Weight\n0.8kg   1kg 1.2kg 1.5kg \n 1490  2203  2424  1383 \n",
        "> ",
        "xtabs(choice ~ ScreenSize, data=laptops)",
        "ScreenSize\n  12   13   14   16 \n 838 1644 2496 2522 \n",
        "> ",
        "laptops.mlogit <- dfidx(laptops, idx = list(c(\"ques\", \"resp.id\"), \"alt\"), drop.index=F, levels=c(1,2,3,4))",
        "> ",
        "lm1 <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize , data = laptops.mlogit)",
        "> ",
        "summary(lm1)",
        "\nCall:\nmlogit(formula = choice ~ Price + RAM + Memory + Processor + \n    Weight + ScreenSize, data = laptops.mlogit, method = \"nr\")\n\nFrequencies of alternatives:choice\n      1       2       3       4 \n0.24573 0.25267 0.25227 0.24933 \n\nnr method\n6 iterations, 0h:0m:7s \ng'(-H)^-1g = 2.15E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n               Estimate Std. Error  z-value  Pr(>|z|)    \n(Intercept):2  0.045634   0.040334   1.1314 0.2578870    \n(Intercept):3  0.058009   0.040377   1.4367",
        " 0.1508084    \n(Intercept):4  0.032013   0.040412   0.7922 0.4282674    \nPrice1        -0.446694   0.045273  -9.8667 < 2.2e-16 ***\nPrice1.5      -0.571078   0.045618 -12.5186 < 2.2e-16 ***\nPrice2        -0.953224   0.047344 -20.1338 < 2.2e-16 ***\nRAM8GB         0.924216   0.044407  20.8124 < 2.2e-16 ***\nRAM16GB       -0.690506   0.047911 -14.4122 < 2.2e-16 ***\nRAM32GB       -0.813077   0.048592 -16.7326 < 2.2e-16 ***\nMemory256GB   -1.088881   0.049437 -22.0258 < 2.2e-16 ***\nMemory512GB   -1.003092   0.048926",
        " -20.5021 < 2.2e-16 ***\nMemory1T       1.027503   0.043944  23.3822 < 2.2e-16 ***\nProcessori5    0.190759   0.043949   4.3405 1.421e-05 ***\nProcessori7   -0.185945   0.045471  -4.0893 4.327e-05 ***\nProcessori9   -1.113779   0.050210 -22.1825 < 2.2e-16 ***\nWeight1kg      0.767455   0.047487  16.1615 < 2.2e-16 ***\nWeight1.2kg    0.942015   0.047594  19.7926 < 2.2e-16 ***\nWeight1.5kg   -0.175095   0.050035  -3.4994 0.0004663 ***\nScreenSize13   1.124687   0.054422  20.6660 < 2.2e-16 ***\nScreenSize14   1.932395   0.054466",
        "  35.4788 < 2.2e-16 ***\nScreenSize16   2.007688   0.054852  36.6019 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nLog-Likelihood: -6750.6\nMcFadden R^2:  0.3507 \nLikelihood ratio test : chisq = 7292.3 (p.value = < 2.22e-16)\n",
        "> ",
        "lm2 <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit)",
        "> ",
        "summary(lm2)",
        "\nCall:\nmlogit(formula = choice ~ Price + RAM + Memory + Processor + \n    Weight + ScreenSize | -1, data = laptops.mlogit, method = \"nr\")\n\nFrequencies of alternatives:choice\n      1       2       3       4 \n0.24573 0.25267 0.25227 0.24933 \n\nnr method\n6 iterations, 0h:0m:5s \ng'(-H)^-1g = 2.11E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error  z-value  Pr(>|z|)    \nPrice1       -0.446283   0.045266  -9.8591 < 2.2e-16 ***\nPrice1.5     -0.571655   0.045612",
        " -12.5331 < 2.2e-16 ***\nPrice2       -0.953178   0.047342 -20.1337 < 2.2e-16 ***\nRAM8GB        0.923871   0.044399  20.8084 < 2.2e-16 ***\nRAM16GB      -0.689757   0.047904 -14.3989 < 2.2e-16 ***\nRAM32GB      -0.812848   0.048587 -16.7296 < 2.2e-16 ***\nMemory256GB  -1.088117   0.049424 -22.0160 < 2.2e-16 ***\nMemory512GB  -1.002787   0.048897 -20.5082 < 2.2e-16 ***\nMemory1T      1.027356   0.043936  23.3829 < 2.2e-16 ***\nProcessori5   0.190622   0.043945   4.3377 1.440e-05 ***\nProcessori7  -0.185582   0.045461",
        "  -4.0822 4.461e-05 ***\nProcessori9  -1.113668   0.050205 -22.1823 < 2.2e-16 ***\nWeight1kg     0.767671   0.047483  16.1673 < 2.2e-16 ***\nWeight1.2kg   0.942085   0.047585  19.7978 < 2.2e-16 ***\nWeight1.5kg  -0.174588   0.050011  -3.4910 0.0004813 ***\nScreenSize13  1.123946   0.054412  20.6561 < 2.2e-16 ***\nScreenSize14  1.931256   0.054448  35.4697 < 2.2e-16 ***\nScreenSize16  2.006770   0.054839  36.5938 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nLog-Likelihood: -6751.7\n",
        "> ",
        "",
        "> ",
        "lrtest(lm1, lm2)",
        "Likelihood ratio test\n\nModel 1: choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize\nModel 2: choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | \n    -1\n  #Df  LogLik Df Chisq Pr(>Chisq)\n1  21 -6750.6                    \n2  18 -6751.7 -3 2.296     0.5133\n",
        "> ",
        "print(\"\")",
        "[1] \"\"\n",
        "> ",
        "print(\"Analyze model with price as qualitative vs quantitative\")",
        "[1] \"Analyze model with price as qualitative vs quantitative\"\n",
        "> ",
        "lm3 <- mlogit(choice ~  as.numeric(as.character(Price)) + RAM + Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit)",
        "> ",
        "summary(lm3)",
        "\nCall:\nmlogit(formula = choice ~ as.numeric(as.character(Price)) + RAM + \n    Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n      1       2       3       4 \n0.24573 0.25267 0.25227 0.24933 \n\nnr method\n6 iterations, 0h:0m:4s \ng'(-H)^-1g = 1.93E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                                 Estimate Std. Error  z-value  Pr(>|z|)    \nas.numeric(as.character(Price)) -0.665861",
        "   0.034044 -19.5591 < 2.2e-16 ***\nRAM8GB                           0.921750   0.044362  20.7778 < 2.2e-16 ***\nRAM16GB                         -0.686156   0.047856 -14.3379 < 2.2e-16 ***\nRAM32GB                         -0.808538   0.048525 -16.6623 < 2.2e-16 ***\nMemory256GB                     -1.085008   0.049381 -21.9724 < 2.2e-16 ***\nMemory512GB                     -1.000592   0.048848 -20.4840 < 2.2e-16 ***\nMemory1T                         1.024403   0.043871  23.3505 < 2.2e-16 ***\nProcessori5                    ",
        "  0.191557   0.043908   4.3627 1.285e-05 ***\nProcessori7                     -0.184549   0.045405  -4.0645 4.813e-05 ***\nProcessori9                     -1.109449   0.050105 -22.1423 < 2.2e-16 ***\nWeight1kg                        0.765588   0.047406  16.1497 < 2.2e-16 ***\nWeight1.2kg                      0.944819   0.047520  19.8824 < 2.2e-16 ***\nWeight1.5kg                     -0.173441   0.049971  -3.4708 0.0005188 ***\nScreenSize13                     1.118958   0.054337  20.5929 < 2.2e-16 ***\nScreenSize14                   ",
        "  1.922437   0.054318  35.3925 < 2.2e-16 ***\nScreenSize16                     1.999807   0.054757  36.5216 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nLog-Likelihood: -6768.6\n",
        "> ",
        "lrtest(lm3, lm2)",
        "Likelihood ratio test\n\nModel 1: choice ~ as.numeric(as.character(Price)) + RAM + Memory + Processor + \n    Weight + ScreenSize | -1\nModel 2: choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | \n    -1\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \n1  16 -6768.6                         \n2  18 -6751.7  2 33.747  4.698e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
        "> ",
        "coef(lm3)[\"RAM16GB\"]/(coef(lm3)[\"as.numeric(as.character(Price))\"]/1000)",
        " RAM16GB \n1030.479 \n",
        "> ",
        "# print(\"since our p-value is smaller than our significance level(0.05), we cannot use price as quantitative variable. This means that we cannot analyze the willingness-to-pay for each level's attribute.\")",
        "> ",
        "#adding index",
        "> ",
        "laptops.chosen <- filter(laptops,laptops$choice == \"1\")",
        "> ",
        "laptops.indexed <- laptops.chosen",
        "> ",
        "laptops.indexed$id <- paste(as.character(laptops.indexed$Price),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$RAM),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$Memory),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$Processor),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$Weight),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$ScreenSize), sep = \"\")",
        "> ",
        "",
        "> ",
        "# Profiles more \"popular\" (top chosen)",
        "> ",
        "freqtable <- table(laptops.indexed$id)",
        "> ",
        "df <- as.data.frame.table(freqtable)",
        "> ",
        "df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)",
        "> ",
        "df <- df[1:15,]",
        "> ",
        "df <- transform(df, Profiles=reorder(Profiles, -Freq)) ",
        "> ",
        "theme_set(theme_classic())",
        "> ",
        "## Plot",
        "> ",
        "g <- ggplot(df, aes(Profiles, Freq))",
        "> ",
        "g + geom_bar(stat=\"identity\", width = 0.5, fill=\"tomato2\") + ",
        "+ ",
        "  labs(title=\"Profiles counting\", ",
        "+ ",
        "       caption=\"Frequency of profiles\") +",
        "+ ",
        "  theme(axis.text.x = element_text(angle=65, vjust=0.6))",
        "> ",
        "",
        "> ",
        "attributes <- list(Price=names(table(laptops.mlogit$Price)),",
        "+ ",
        "                   RAM=names(table(laptops.mlogit$RAM)),",
        "+ ",
        "                   Memory=names(table(laptops.mlogit$Memory)),",
        "+ ",
        "                   Processor=names(table(laptops.mlogit$Processor)),",
        "+ ",
        "                   Weight=names(table(laptops.mlogit$Weight)),",
        "+ ",
        "                   ScreenSize=names(table(laptops.mlogit$ScreenSize)))",
        "> ",
        "allDesign <- expand.grid(attributes) ",
        "> ",
        "",
        "> ",
        "ProductSelection <- function(Price,RAM,Memory,Processor,Weight,ScreenSize){",
        "+ ",
        "  ram <-  paste(as.character(RAM), \"GB\", sep = \"\")",
        "+ ",
        "  if(Memory==1) memory <- paste(as.character(Memory), \"T\", sep = \"\") else memory <-  paste(as.character(Memory), \"GB\", sep = \"\")",
        "+ ",
        "  processor <-  paste('i',as.character(Processor), sep = \"\")",
        "+ ",
        "  weight <-  paste(as.character(Weight), \"kg\", sep = \"\")",
        "+ ",
        "  ",
        "+ ",
        "  return(filter(allDesign, Price == {{Price}}, RAM == {{ram}}, Memory == {{ memory }}, Processor == {{ processor }}, Weight == {{ weight }}, ScreenSize == {{ ScreenSize }}))",
        "+ ",
        "}",
        "> ",
        "#Entry market",
        "> ",
        "entry1 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 126,Processor = 3, Weight = 0.8,ScreenSize = 12)",
        "> ",
        "entry2 <- ProductSelection(Price = 1, RAM = 4,Memory = 126,Processor = 3, Weight = 1.2,ScreenSize = 13)",
        "> ",
        "entry3 <- ProductSelection(Price = 0.7, RAM = 8,Memory = 126,Processor = 3, Weight = 1.2,ScreenSize = 14)",
        "> ",
        "entry4 <- ProductSelection(Price = 0.7, RAM = 8,Memory = 1,Processor = 3, Weight = 1.2,ScreenSize = 16)",
        "> ",
        "",
        "> ",
        "#Mid market",
        "> ",
        "mid1 <- ProductSelection(Price = 1, RAM = 8,Memory = 256,Processor = 5, Weight = 1, ScreenSize = 13)",
        "> ",
        "mid2 <- ProductSelection(Price = 1.5, RAM = 16,Memory = 512,Processor = 7, Weight = 1.5,ScreenSize = 16)",
        "> ",
        "mid3 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 126,Processor = 5, Weight = 1.2,ScreenSize = 14)",
        "> ",
        "",
        "> ",
        "#High end market",
        "> ",
        "high1 <- ProductSelection(Price = 2, RAM = 16,Memory = 512, Processor = 7, Weight = 1.2,ScreenSize = 16)",
        "> ",
        "high2 <- ProductSelection(Price = 2, RAM = 32,Memory = 1, Processor = 9, Weight = 1.5,ScreenSize = 14)",
        "> ",
        "high3 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 1,Processor = 9, Weight = 1,ScreenSize = 16)",
        "> ",
        "",
        "> ",
        "profiles <- rbind(entry1, entry2,entry3, entry4, mid1, mid2, mid3, high1, high2, high3)",
        "> ",
        "print(profiles)",
        "> ",
        "predict.mnl <- function(model, data) {",
        "+ ",
        "  # Function for predicting preference shares from a MNL model ",
        "+ ",
        "  # model: mlogit object returned by mlogit()",
        "+ ",
        "  # data: a data frame containing the set of designs for which you want to ",
        "+ ",
        "  #       predict shares.  Same format at the data used to estimate model. ",
        "+ ",
        "  data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]",
        "+ ",
        "  logitUtility <- data.model%*%model$coef",
        "+ ",
        "  share <- exp(logitUtility)/sum(exp(logitUtility))",
        "+ ",
        "  cbind(share, data)",
        "+ ",
        "}",
        "> ",
        "predict.mnl(lm2, profiles)",
        "> ",
        "knitr::opts_chunk$set(echo = TRUE)",
        "> ",
        "library(mlogit)",
        "> ",
        "library(dplyr)",
        "> ",
        "library(ggplot2)",
        "> ",
        "library(MASS)",
        "> ",
        "library(lattice)",
        "> ",
        "library(parallel)",
        "> ",
        "laptops = read.csv(\"Dataset/laptops.csv\", sep=\";\")",
        "> ",
        "",
        "> ",
        "laptops$Price       <- as.factor(laptops$Price)",
        "> ",
        "laptops$RAM         <- factor(laptops$RAM, levels=c( \"4GB\", \"8GB\", \"16GB\", \"32GB\" ))",
        "> ",
        "laptops$Memory      <- factor(laptops$Memory, levels=c( \"126GB\", \"256GB\", \"512GB\", \"1T\" ))",
        "> ",
        "laptops$Processor   <- factor(laptops$Processor, levels=c( \"i3\", \"i5\", \"i7\", \"i9\" ))",
        "> ",
        "laptops$Weight      <- factor(laptops$Weight, levels=c( \"0.8kg\", \"1kg\", \"1.2kg\", \"1.5kg\" ))",
        "> ",
        "laptops$ScreenSize  <- as.factor(laptops$ScreenSize) ",
        "> ",
        "laptops$alt         <- factor(laptops$alt, levels=c(\"1\", \"2\", \"3\", \"4\"))",
        "> ",
        "summary(laptops)",
        "    resp.id            ques      alt      Price        RAM         Memory     Processor   Weight     ScreenSize\n Min.   :  1.00   Min.   :   1   1:7500   0.7:7516   4GB :7582   126GB:7432   i3:7491   0.8kg:7464   12:7584   \n 1st Qu.: 75.75   1st Qu.:1876   2:7500   1  :7411   8GB :7319   256GB:7596   i5:7580   1kg  :7372   13:7487   \n Median :150.50   Median :3750   3:7500   1.5:7545   16GB:7609   512GB:7524   i7:7352   1.2kg:7584   14:7509   \n Mean   :150.50   Mean   :3750   4:7500   2  :7528   32GB:7490  ",
        " 1T   :7448   i9:7577   1.5kg:7580   16:7420   \n 3rd Qu.:225.25   3rd Qu.:5625                                                                                 \n Max.   :300.00   Max.   :7500                                                                                 \n     choice    \n Min.   :0.00  \n 1st Qu.:0.00  \n Median :0.00  \n Mean   :0.25  \n 3rd Qu.:0.25  \n Max.   :1.00  \n",
        "> ",
        "# To check balance, he used this function",
        "> ",
        "sapply(laptops, table)",
        "$resp.id\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n 59  60  61  62  63  64  65  66  67 ",
        " 68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n 88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ",
        "137 138 139 140 141 142 143 144 145 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 \n100 100 ",
        "100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 \n100 100 100 100 100 100 100 100 100 100 100 100 100 ",
        "100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 \n100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n291 292 293 294 295 296 297 298 299 300 \n100 100 100 100 100 100 100 100 100 100 \n\n$ques\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18   19   20   21   22   23 \n   4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n  24   25   26   27   28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   43   44   45   46 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n  47   48   49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64   65   66   67   68   69 \n   4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4 \n  70   71   72   73   74   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   90   91   92 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n  93   94   95   96   97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4 \n 116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 162  163  164  165  166  167  168  169  170 ",
        " 171  172  173  174  175  176  177  178  179  180  181  182  183  184 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226 ",
        " 227  228  229  230 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274  275  276 \n   4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4 \n 323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 369  370  371 ",
        " 372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 415  416  417  418  419  420  421  422  423  424  425  426  427 ",
        " 428  429  430  431  432  433  434  435  436  437 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 438  439  440  441  442  443  444  445  446  447  448  449  450  451  452  453  454  455  456  457  458  459  460 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483 ",
        "\n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 484  485  486  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 507  508  509  510  511  512  513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528  529 \n   4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4 \n 530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547  548  549  550  551  552 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 553  554  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4 \n 576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592  593  594  595  596  597  598 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616  617  618  619  620  621 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 622  623  624  625  626  627  628 ",
        " 629  630  631  632  633  634  635  636  637  638  639  640  641  642  643  644 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 645  646  647  648  649  650  651  652  653  654  655  656  657  658  659  660  661  662  663  664  665  666  667 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 668  669  670  671  672  673  674  675  676  677  678  679  680  681  682  683  684 ",
        " 685  686  687  688  689  690 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 691  692  693  694  695  696  697  698  699  700  701  702  703  704  705  706  707  708  709  710  711  712  713 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 714  715  716  717  718  719  720  721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752  753  754  755  756  757  758  759 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 760  761  762  763  764  765  766  767  768  769  770  771  772  773  774  775  776  777  778  779  780  781  782 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4 \n 783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 806  807  808  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824  825  826  827  828 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 829 ",
        " 830  831  832  833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  871  872  873  874 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 875  876  877  878  879  880  881  882  883  884  885 ",
        " 886  887  888  889  890  891  892  893  894  895  896  897 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 898  899  900  901  902  903  904  905  906  907  908  909  910  911  912  913  914  915  916  917  918  919  920 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941 ",
        " 942  943 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966 \n   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 967  968  969  970  971  972  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989 \n   4    4    4    4    4    4    4    4 ",
        "   4    4    4    4    4    4    4    4    4    4    4    4    4    4    4 \n 990  991  992  993  994  995  996  997  998  999 1000 \n   4    4    4    4    4    4    4    4    4    4    4 \n [ reached getOption(\"max.print\") -- omitted 6500 entries ]\n\n$alt\n\n   1    2    3    4 \n7500 7500 7500 7500 \n\n$Price\n\n 0.7    1  1.5    2 \n7516 7411 7545 7528 \n\n$RAM\n\n 4GB  8GB 16GB 32GB \n7582 7319 7609 7490 \n\n$Memory\n\n126GB 256GB 512GB    1T \n 7432  7596  7524  7448 \n\n$Processor\n\n  i3   i5   i7   i9 \n7491 7580 7352 7577 \n",
        "\n$Weight\n\n0.8kg   1kg 1.2kg 1.5kg \n 7464  7372  7584  7580 \n\n$ScreenSize\n\n  12   13   14   16 \n7584 7487 7509 7420 \n\n$choice\n\n    0     1 \n22500  7500 \n\n",
        "> ",
        "xtabs(choice ~ Price, data=laptops)",
        "Price\n 0.7    1  1.5    2 \n2338 1895 1773 1494 \n",
        "> ",
        "xtabs(choice ~ RAM, data=laptops)",
        "RAM\n 4GB  8GB 16GB 32GB \n1988 2865 1347 1300 \n",
        "> ",
        "xtabs(choice ~ Memory, data=laptops)",
        "Memory\n126GB 256GB 512GB    1T \n 2043  1127  1153  3177 \n",
        "> ",
        "xtabs(choice ~ Processor, data=laptops)",
        "Processor\n  i3   i5   i7   i9 \n2121 2329 1861 1189 \n",
        "> ",
        "xtabs(choice ~ Weight, data=laptops)",
        "Weight\n0.8kg   1kg 1.2kg 1.5kg \n 1490  2203  2424  1383 \n",
        "> ",
        "xtabs(choice ~ ScreenSize, data=laptops)",
        "ScreenSize\n  12   13   14   16 \n 838 1644 2496 2522 \n",
        "> ",
        "laptops.mlogit <- dfidx(laptops, idx = list(c(\"ques\", \"resp.id\"), \"alt\"), drop.index=F, levels=c(1,2,3,4))",
        "> ",
        "lm1 <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize , data = laptops.mlogit)",
        "> ",
        "summary(lm1)",
        "\nCall:\nmlogit(formula = choice ~ Price + RAM + Memory + Processor + \n    Weight + ScreenSize, data = laptops.mlogit, method = \"nr\")\n\nFrequencies of alternatives:choice\n      1       2       3       4 \n0.24573 0.25267 0.25227 0.24933 \n\nnr method\n6 iterations, 0h:0m:4s \ng'(-H)^-1g = 2.15E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n               Estimate Std. Error  z-value  Pr(>|z|)    \n(Intercept):2  0.045634   0.040334   1.1314 0.2578870    \n(Intercept):3  0.058009   0.040377   1.4367",
        " 0.1508084    \n(Intercept):4  0.032013   0.040412   0.7922 0.4282674    \nPrice1        -0.446694   0.045273  -9.8667 < 2.2e-16 ***\nPrice1.5      -0.571078   0.045618 -12.5186 < 2.2e-16 ***\nPrice2        -0.953224   0.047344 -20.1338 < 2.2e-16 ***\nRAM8GB         0.924216   0.044407  20.8124 < 2.2e-16 ***\nRAM16GB       -0.690506   0.047911 -14.4122 < 2.2e-16 ***\nRAM32GB       -0.813077   0.048592 -16.7326 < 2.2e-16 ***\nMemory256GB   -1.088881   0.049437 -22.0258 < 2.2e-16 ***\nMemory512GB   -1.003092   0.048926",
        " -20.5021 < 2.2e-16 ***\nMemory1T       1.027503   0.043944  23.3822 < 2.2e-16 ***\nProcessori5    0.190759   0.043949   4.3405 1.421e-05 ***\nProcessori7   -0.185945   0.045471  -4.0893 4.327e-05 ***\nProcessori9   -1.113779   0.050210 -22.1825 < 2.2e-16 ***\nWeight1kg      0.767455   0.047487  16.1615 < 2.2e-16 ***\nWeight1.2kg    0.942015   0.047594  19.7926 < 2.2e-16 ***\nWeight1.5kg   -0.175095   0.050035  -3.4994 0.0004663 ***\nScreenSize13   1.124687   0.054422  20.6660 < 2.2e-16 ***\nScreenSize14   1.932395   0.054466",
        "  35.4788 < 2.2e-16 ***\nScreenSize16   2.007688   0.054852  36.6019 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nLog-Likelihood: -6750.6\nMcFadden R^2:  0.3507 \nLikelihood ratio test : chisq = 7292.3 (p.value = < 2.22e-16)\n",
        "> ",
        "lm2 <- mlogit(choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit)",
        "> ",
        "summary(lm2)",
        "\nCall:\nmlogit(formula = choice ~ Price + RAM + Memory + Processor + \n    Weight + ScreenSize | -1, data = laptops.mlogit, method = \"nr\")\n\nFrequencies of alternatives:choice\n      1       2       3       4 \n0.24573 0.25267 0.25227 0.24933 \n\nnr method\n6 iterations, 0h:0m:5s \ng'(-H)^-1g = 2.11E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error  z-value  Pr(>|z|)    \nPrice1       -0.446283   0.045266  -9.8591 < 2.2e-16 ***\nPrice1.5     -0.571655   0.045612",
        " -12.5331 < 2.2e-16 ***\nPrice2       -0.953178   0.047342 -20.1337 < 2.2e-16 ***\nRAM8GB        0.923871   0.044399  20.8084 < 2.2e-16 ***\nRAM16GB      -0.689757   0.047904 -14.3989 < 2.2e-16 ***\nRAM32GB      -0.812848   0.048587 -16.7296 < 2.2e-16 ***\nMemory256GB  -1.088117   0.049424 -22.0160 < 2.2e-16 ***\nMemory512GB  -1.002787   0.048897 -20.5082 < 2.2e-16 ***\nMemory1T      1.027356   0.043936  23.3829 < 2.2e-16 ***\nProcessori5   0.190622   0.043945   4.3377 1.440e-05 ***\nProcessori7  -0.185582   0.045461",
        "  -4.0822 4.461e-05 ***\nProcessori9  -1.113668   0.050205 -22.1823 < 2.2e-16 ***\nWeight1kg     0.767671   0.047483  16.1673 < 2.2e-16 ***\nWeight1.2kg   0.942085   0.047585  19.7978 < 2.2e-16 ***\nWeight1.5kg  -0.174588   0.050011  -3.4910 0.0004813 ***\nScreenSize13  1.123946   0.054412  20.6561 < 2.2e-16 ***\nScreenSize14  1.931256   0.054448  35.4697 < 2.2e-16 ***\nScreenSize16  2.006770   0.054839  36.5938 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nLog-Likelihood: -6751.7\n",
        "> ",
        "",
        "> ",
        "lrtest(lm1, lm2)",
        "Likelihood ratio test\n\nModel 1: choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize\nModel 2: choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | \n    -1\n  #Df  LogLik Df Chisq Pr(>Chisq)\n1  21 -6750.6                    \n2  18 -6751.7 -3 2.296     0.5133\n",
        "> ",
        "print(\"\")",
        "[1] \"\"\n",
        "> ",
        "print(\"Analyze model with price as qualitative vs quantitative\")",
        "[1] \"Analyze model with price as qualitative vs quantitative\"\n",
        "> ",
        "lm3 <- mlogit(choice ~  as.numeric(as.character(Price)) + RAM + Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit)",
        "> ",
        "summary(lm3)",
        "\nCall:\nmlogit(formula = choice ~ as.numeric(as.character(Price)) + RAM + \n    Memory + Processor + Weight + ScreenSize | -1, data = laptops.mlogit, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n      1       2       3       4 \n0.24573 0.25267 0.25227 0.24933 \n\nnr method\n6 iterations, 0h:0m:8s \ng'(-H)^-1g = 1.93E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                                 Estimate Std. Error  z-value  Pr(>|z|)    \nas.numeric(as.character(Price)) -0.665861",
        "   0.034044 -19.5591 < 2.2e-16 ***\nRAM8GB                           0.921750   0.044362  20.7778 < 2.2e-16 ***\nRAM16GB                         -0.686156   0.047856 -14.3379 < 2.2e-16 ***\nRAM32GB                         -0.808538   0.048525 -16.6623 < 2.2e-16 ***\nMemory256GB                     -1.085008   0.049381 -21.9724 < 2.2e-16 ***\nMemory512GB                     -1.000592   0.048848 -20.4840 < 2.2e-16 ***\nMemory1T                         1.024403   0.043871  23.3505 < 2.2e-16 ***\nProcessori5                    ",
        "  0.191557   0.043908   4.3627 1.285e-05 ***\nProcessori7                     -0.184549   0.045405  -4.0645 4.813e-05 ***\nProcessori9                     -1.109449   0.050105 -22.1423 < 2.2e-16 ***\nWeight1kg                        0.765588   0.047406  16.1497 < 2.2e-16 ***\nWeight1.2kg                      0.944819   0.047520  19.8824 < 2.2e-16 ***\nWeight1.5kg                     -0.173441   0.049971  -3.4708 0.0005188 ***\nScreenSize13                     1.118958   0.054337  20.5929 < 2.2e-16 ***\nScreenSize14                   ",
        "  1.922437   0.054318  35.3925 < 2.2e-16 ***\nScreenSize16                     1.999807   0.054757  36.5216 < 2.2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nLog-Likelihood: -6768.6\n",
        "> ",
        "lrtest(lm3, lm2)",
        "Likelihood ratio test\n\nModel 1: choice ~ as.numeric(as.character(Price)) + RAM + Memory + Processor + \n    Weight + ScreenSize | -1\nModel 2: choice ~ Price + RAM + Memory + Processor + Weight + ScreenSize | \n    -1\n  #Df  LogLik Df",
        "\n",
        "\n",
        "> ",
        "coef(lm3)[\"RAM16GB\"]/(coef(lm3)[\"as.numeric(as.character(Price))\"]/1000)",
        " RAM16GB \n1030.479 \n",
        "> ",
        "# print(\"since our p-value is smaller than our significance level(0.05), we cannot use price as quantitative variable. This means that we cannot analyze the willingness-to-pay for each level's attribute.\")",
        "> ",
        "#adding index",
        "> ",
        "laptops.chosen <- filter(laptops,laptops$choice == \"1\")",
        "> ",
        "laptops.indexed <- laptops.chosen",
        "> ",
        "laptops.indexed$id <- paste(as.character(laptops.indexed$Price),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$RAM),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$Memory),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$Processor),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$Weight),\"-\",",
        "+ ",
        "                            as.character(laptops.indexed$ScreenSize), sep = \"\")",
        "> ",
        "",
        "> ",
        "# Profiles more \"popular\" (top chosen)",
        "> ",
        "freqtable <- table(laptops.indexed$id)",
        "> ",
        "df <- as.data.frame.table(freqtable)",
        "> ",
        "df <- df %>% as.data.frame() %>% top_n(15, Freq) %>% rename(Profiles = Var1)",
        "> ",
        "df <- df[1:15,]",
        "> ",
        "df <- transform(df, Profiles=reorder(Profiles, -Freq)) ",
        "> ",
        "theme_set(theme_classic())",
        "> ",
        "## Plot",
        "> ",
        "g <- ggplot(df, aes(Profiles, Freq))",
        "> ",
        "g + geom_bar(stat=\"identity\", width = 0.5, fill=\"tomato2\") + ",
        "+ ",
        "  labs(title=\"Profiles counting\", ",
        "+ ",
        "       caption=\"Frequency of profiles\") +",
        "+ ",
        "  theme(axis.text.x = element_text(angle=65, vjust=0.6))",
        "> ",
        "",
        "> ",
        "attributes <- list(Price=names(table(laptops.mlogit$Price)),",
        "+ ",
        "                   RAM=names(table(laptops.mlogit$RAM)),",
        "+ ",
        "                   Memory=names(table(laptops.mlogit$Memory)),",
        "+ ",
        "                   Processor=names(table(laptops.mlogit$Processor)),",
        "+ ",
        "                   Weight=names(table(laptops.mlogit$Weight)),",
        "+ ",
        "                   ScreenSize=names(table(laptops.mlogit$ScreenSize)))",
        "> ",
        "allDesign <- expand.grid(attributes) ",
        "> ",
        "",
        "> ",
        "ProductSelection <- function(Price,RAM,Memory,Processor,Weight,ScreenSize){",
        "+ ",
        "  ram <-  paste(as.character(RAM), \"GB\", sep = \"\")",
        "+ ",
        "  if(Memory==1) memory <- paste(as.character(Memory), \"T\", sep = \"\") else memory <-  paste(as.character(Memory), \"GB\", sep = \"\")",
        "+ ",
        "  processor <-  paste('i',as.character(Processor), sep = \"\")",
        "+ ",
        "  weight <-  paste(as.character(Weight), \"kg\", sep = \"\")",
        "+ ",
        "  ",
        "+ ",
        "  return(filter(allDesign, Price == {{Price}}, RAM == {{ram}}, Memory == {{ memory }}, Processor == {{ processor }}, Weight == {{ weight }}, ScreenSize == {{ ScreenSize }}))",
        "+ ",
        "}",
        "> ",
        "#Entry market",
        "> ",
        "entry1 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 126,Processor = 3, Weight = 0.8,ScreenSize = 12)",
        "> ",
        "entry2 <- ProductSelection(Price = 1, RAM = 4,Memory = 126,Processor = 3, Weight = 1.2,ScreenSize = 13)",
        "> ",
        "entry3 <- ProductSelection(Price = 0.7, RAM = 8,Memory = 126,Processor = 3, Weight = 1.2,ScreenSize = 14)",
        "> ",
        "entry4 <- ProductSelection(Price = 0.7, RAM = 8,Memory = 1,Processor = 3, Weight = 1.2,ScreenSize = 16)",
        "> ",
        "",
        "> ",
        "#Mid market",
        "> ",
        "mid1 <- ProductSelection(Price = 1, RAM = 8,Memory = 256,Processor = 5, Weight = 1, ScreenSize = 13)",
        "> ",
        "mid2 <- ProductSelection(Price = 1.5, RAM = 16,Memory = 512,Processor = 7, Weight = 1.5,ScreenSize = 16)",
        "> ",
        "mid3 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 126,Processor = 5, Weight = 1.2,ScreenSize = 14)",
        "> ",
        "",
        "> ",
        "#High end market",
        "> ",
        "high1 <- ProductSelection(Price = 2, RAM = 16,Memory = 512, Processor = 7, Weight = 1.2,ScreenSize = 16)",
        "> ",
        "high2 <- ProductSelection(Price = 2, RAM = 32,Memory = 1, Processor = 9, Weight = 1.5,ScreenSize = 14)",
        "> ",
        "high3 <- ProductSelection(Price = 0.7, RAM = 4,Memory = 1,Processor = 9, Weight = 1,ScreenSize = 16)",
        "> ",
        "",
        "> ",
        "profiles <- rbind(entry1, entry2,entry3, entry4, mid1, mid2, mid3, high1, high2, high3)",
        "> ",
        "print(profiles)",
        "> ",
        "predict.mnl <- function(model, data) {",
        "+ ",
        "  # Function for predicting preference shares from a MNL model ",
        "+ ",
        "  # model: mlogit object returned by mlogit()",
        "+ ",
        "  # data: a data frame containing the set of designs for which you want to ",
        "+ ",
        "  #       predict shares.  Same format at the data used to estimate model. ",
        "+ ",
        "  data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]",
        "+ ",
        "  logitUtility <- data.model%*%model$coef",
        "+ ",
        "  share <- exp(logitUtility)/sum(exp(logitUtility))",
        "+ ",
        "  cbind(share, data)",
        "+ ",
        "}",
        "> ",
        "predict.mnl(lm2, profiles)",
        "\nRestarting R session...\n\n"
    ]
}